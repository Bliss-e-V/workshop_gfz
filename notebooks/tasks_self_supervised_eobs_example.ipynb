{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸŒ¦ï¸ Self-Supervised Learning Workshop\n",
        "## Learning Weather Patterns from E-OBS Data\n",
        "\n",
        "Welcome to this hands-on workshop on self-supervised learning for climate data! \n",
        "\n",
        "### ğŸ¯ Learning Objectives\n",
        "By the end of this workshop, you will:\n",
        "- Understand self-supervised learning concepts\n",
        "- Implement temporal prediction for weather forecasting\n",
        "- Build masked modeling for data reconstruction\n",
        "- Use PyTorch Lightning for clean ML code\n",
        "- Visualize and evaluate model performance\n",
        "\n",
        "### ğŸ§  What is Self-Supervised Learning?\n",
        "Self-supervised learning allows models to learn from data without manual labels by creating learning tasks from the data itself:\n",
        "\n",
        "1. **Temporal Prediction**: \"Given 3 days of weather, predict tomorrow\"\n",
        "2. **Masked Modeling**: \"Fill in missing parts of weather maps\"\n",
        "\n",
        "### ğŸ“Š Dataset: E-OBS Precipitation\n",
        "- **Source**: European daily precipitation observations\n",
        "- **Coverage**: 1950-2024, 0.25Â° grid resolution\n",
        "- **Task**: Learn spatial and temporal patterns\n",
        "\n",
        "Let's dive in! ğŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ Workshop Setup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Hide unnecessary warnings for cleaner output\n",
        "\n",
        "# Essential imports\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Check device availability\n",
        "def get_device_info():\n",
        "    \"\"\"Get information about available compute devices\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"ğŸ® GPU: {torch.cuda.get_device_name()}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "        print(\"ğŸ Apple Silicon GPU (MPS) available\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"ğŸ’» Using CPU\")\n",
        "    \n",
        "    return device\n",
        "\n",
        "print(\"ğŸŒ Welcome to the Self-Supervised Learning Workshop!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "device = get_device_info()\n",
        "print(f\"Selected device: {device}\")\n",
        "print(\"\\nâœ… Setup complete - let's start learning!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“ Workshop Learning Path\n",
        "\n",
        "We'll explore two fundamental self-supervised learning approaches:\n",
        "\n",
        "### 1ï¸âƒ£ **Temporal Prediction**\n",
        "- **Goal**: Predict tomorrow's weather from past 3 days\n",
        "- **Learning**: Temporal patterns and sequences\n",
        "- **Applications**: Weather forecasting, time series prediction\n",
        "\n",
        "### 2ï¸âƒ£ **Masked Modeling**\n",
        "- **Goal**: Reconstruct missing parts of weather maps\n",
        "- **Learning**: Spatial relationships and patterns\n",
        "- **Applications**: Data imputation, anomaly detection\n",
        "\n",
        "### ğŸ—ï¸ **Technical Architecture**\n",
        "- **Framework**: PyTorch Lightning for clean code\n",
        "- **Models**: Lightweight ConvLSTM and U-Net architectures\n",
        "- **Training**: Fast iterations with early stopping\n",
        "- **Evaluation**: Visual results + quantitative metrics\n",
        "\n",
        "### ğŸ¯ **Workshop Flow**\n",
        "1. Load and explore the precipitation data\n",
        "2. Build temporal prediction pipeline\n",
        "3. Train and evaluate temporal model\n",
        "4. Build masked modeling pipeline\n",
        "5. Train and evaluate masked model\n",
        "6. Compare results and discuss applications\n",
        "\n",
        "Ready to start? Let's load our data! ğŸ“Š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“š Import Essential Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as L\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# ğŸ”§ Import Custom Modules\n",
        "from data_utils import (\n",
        "    EOBSDataLoader,\n",
        "    EOBSTemporalPredictionDataset,\n",
        "    EOBSMaskedModelingDataset,\n",
        "    get_device,\n",
        ")\n",
        "from models import (\n",
        "    TemporalPredictionModel,\n",
        "    MaskedModelingModel,\n",
        "    TemporalPredictionLightningModule,\n",
        "    MaskedModelingLightningModule,\n",
        ")\n",
        "\n",
        "# ğŸ¯ Display Setup Information\n",
        "print(\"ğŸŒ E-OBS Self-Supervised Learning Workshop\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"PyTorch Lightning version: {L.__version__}\")\n",
        "\n",
        "# ğŸ–¥ï¸ Setup Device\n",
        "device = get_device()\n",
        "print(f\"Selected device: {device}\")\n",
        "\n",
        "# ğŸ“Š Configure Matplotlib for Better Plots\n",
        "plt.style.use(\"default\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "plt.rcParams[\"font.size\"] = 10\n",
        "\n",
        "print(\"\\nâœ… All libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. ğŸ“Š Load and Explore E-OBS Precipitation Data\n",
        "\n",
        "Let's start by loading our climate dataset! The E-OBS dataset contains daily precipitation measurements across Europe from 1950 to 2024.\n",
        "\n",
        "### ğŸŒ About the E-OBS Dataset\n",
        "- **Coverage**: All of Europe on a 0.25Â° grid\n",
        "- **Time span**: 1950-2024 (27,000+ days)\n",
        "- **Variables**: Precipitation (mm/day)\n",
        "- **Resolution**: ~25km spatial resolution\n",
        "\n",
        "**Fun fact**: This dataset contains over 2.5 billion data points! ğŸ¤¯\n",
        "\n",
        "### ğŸ¯ Smart Data Loading (New Feature!)\n",
        "Our data loader now **automatically loads subsampled data by default** for faster processing:\n",
        "- **Subsampled datasets** (~2,000 time steps) load first if available\n",
        "- **Full datasets** (27,000+ time steps) are used as fallback\n",
        "- **Configurable**: You can control this behavior with `prefer_subsampled` parameter\n",
        "\n",
        "This makes the workshop run much faster while still demonstrating all the key concepts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸŒ Load E-OBS Climate Data\n",
        "print(\"ğŸ”„ Loading E-OBS precipitation data ...\")\n",
        "eobs_loader = EOBSDataLoader(data_dir=\"../src/data\")\n",
        "\n",
        "print(\"ğŸ“Š Loading precipitation data...\")\n",
        "print(\"   ğŸ¯ Using smart loading: prefer_subsampled=True (default)\")\n",
        "print(\"   ğŸ“ Will try subsampled files first, then fall back to full dataset\")\n",
        "print()\n",
        "\n",
        "eobs_data = {}\n",
        "try:\n",
        "    # Load precipitation mean data - now with subsampling by default!\n",
        "    eobs_data['precipitation_mean'] = eobs_loader.load_precipitation_data(\n",
        "        prefer_subsampled=True,  # Default behavior (explicit for clarity)\n",
        "        subsample_size=2000      # Look for files with ~2000 time steps\n",
        "    )\n",
        "    print(\"âœ… Precipitation data loaded successfully!\")\n",
        "    \n",
        "    # Show what we actually loaded\n",
        "    if 'precipitation_mean' in eobs_data:\n",
        "        data = eobs_data['precipitation_mean']\n",
        "        time_steps = len(data.time) if hasattr(data, 'time') else 'unknown'\n",
        "        print(f\"   ğŸ“Š Loaded dataset with {time_steps} time steps\")\n",
        "        if time_steps < 10000:\n",
        "            print(\"   ğŸš€ Using subsampled data for faster processing!\")\n",
        "        else:\n",
        "            print(\"   ğŸ“š Using full dataset (subsampled version not available)\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading data: {e}\")\n",
        "    # Fallback to full loading if specific method doesn't exist\n",
        "    eobs_data = eobs_loader.load_all_data()\n",
        "    print(\"âœ… Data loaded successfully (fallback method)!\")\n",
        "\n",
        "# ğŸ“Š Extract and Examine Precipitation Data\n",
        "if 'precipitation_mean' in eobs_data:\n",
        "    precip_data = eobs_data['precipitation_mean']\n",
        "    \n",
        "    # DEBUG: Check what type of data we have\n",
        "    print(f\"ğŸ” Data type: {type(precip_data)}\")\n",
        "    if isinstance(precip_data, dict):\n",
        "        print(f\"ğŸ” Dictionary keys: {list(precip_data.keys())}\")\n",
        "        \n",
        "        # Handle nested dictionary structure\n",
        "        if 'precipitation_mean' in precip_data:\n",
        "            # It's a nested dictionary, get the actual precipitation data\n",
        "            precip_data = precip_data['precipitation_mean']\n",
        "            print(f\"ğŸ”§ Extracted nested 'precipitation_mean' data\")\n",
        "        elif 'rr' in precip_data:\n",
        "            precip_data = precip_data['rr']  # Extract the actual dataset\n",
        "            print(f\"ğŸ”§ Extracted 'rr' data\")\n",
        "        elif len(precip_data) == 1:\n",
        "            precip_data = list(precip_data.values())[0]  # Get the single dataset\n",
        "            print(f\"ğŸ”§ Extracted single dataset\")\n",
        "        else:\n",
        "            # Try each key to find the actual dataset\n",
        "            for key, value in precip_data.items():\n",
        "                if hasattr(value, 'rr') or hasattr(value, 'shape'):\n",
        "                    precip_data = value\n",
        "                    print(f\"ğŸ”§ Found dataset under key: '{key}'\")\n",
        "                    break\n",
        "            else:\n",
        "                print(\"âŒ Could not find dataset in nested structure\")\n",
        "                print(f\"Available keys and their types:\")\n",
        "                for k, v in precip_data.items():\n",
        "                    print(f\"   - {k}: {type(v)}\")\n",
        "    \n",
        "    # Check if we now have a valid dataset\n",
        "    print(f\"ğŸ” Final data type: {type(precip_data)}\")\n",
        "    \n",
        "    # Now try to access the data\n",
        "    print(f\"\\nğŸ“Š Dataset Overview:\")\n",
        "    if hasattr(precip_data, 'rr'):\n",
        "        print(f\"   - Shape: {precip_data.rr.shape}\")\n",
        "        print(f\"   - Time range: {str(precip_data.time.min().values)[:10]} to {str(precip_data.time.max().values)[:10]}\")\n",
        "        print(f\"   - Spatial coverage: {precip_data.latitude.min().values:.1f}Â°N to {precip_data.latitude.max().values:.1f}Â°N\")\n",
        "        print(f\"   - Longitude range: {precip_data.longitude.min().values:.1f}Â°E to {precip_data.longitude.max().values:.1f}Â°E\")\n",
        "        print(f\"   - Data range: {precip_data.rr.min().values:.3f} to {precip_data.rr.max().values:.1f} mm/day\")\n",
        "    elif hasattr(precip_data, 'shape'):\n",
        "        # If it's the data array directly\n",
        "        print(f\"   - Shape: {precip_data.shape}\")\n",
        "        print(f\"   - Time range: {str(precip_data.time.min().values)[:10]} to {str(precip_data.time.max().values)[:10]}\")\n",
        "        print(f\"   - Spatial coverage: {precip_data.latitude.min().values:.1f}Â°N to {precip_data.latitude.max().values:.1f}Â°N\")\n",
        "        print(f\"   - Longitude range: {precip_data.longitude.min().values:.1f}Â°E to {precip_data.longitude.max().values:.1f}Â°E\")\n",
        "        print(f\"   - Data range: {precip_data.min().values:.3f} to {precip_data.max().values:.1f} mm/day\")\n",
        "    else:\n",
        "        print(f\"âŒ Cannot access data. Type: {type(precip_data)}\")\n",
        "        if hasattr(precip_data, 'keys'):\n",
        "            print(f\"Available keys: {list(precip_data.keys())}\")\n",
        "        else:\n",
        "            print(f\"Available attributes: {[attr for attr in dir(precip_data) if not attr.startswith('_')][:10]}...\")\n",
        "    \n",
        "    del eobs_data\n",
        "else:\n",
        "    print(\"âŒ Precipitation data not found. Please check data files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ğŸ›ï¸ Data Loading Options\n",
        "\n",
        "You can control the data loading behavior:\n",
        "\n",
        "```python\n",
        "# Default: Load subsampled data first (recommended for workshops)\n",
        "eobs_loader.load_precipitation_data(prefer_subsampled=True, subsample_size=2000)\n",
        "\n",
        "# Force full dataset (if you have time and computational resources)\n",
        "eobs_loader.load_precipitation_data(prefer_subsampled=False)\n",
        "\n",
        "# Use quick_load_eobs function with same options\n",
        "from data_utils import quick_load_eobs\n",
        "data = quick_load_eobs(prefer_subsampled=True, subsample_size=2000)\n",
        "```\n",
        "\n",
        "**ğŸ’¡ Tip**: Start with subsampled data for faster iteration, then switch to full dataset for final results!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. ğŸ¨ Visualize Sample Data & Task 1\n",
        "\n",
        "Let's explore the precipitation patterns across Europe! Understanding your data is the first step in any ML project.\n",
        "\n",
        "### ğŸ¤” What should we look for?\n",
        "- **Spatial patterns**: Where does it rain most?\n",
        "- **Seasonal variations**: Different weather patterns\n",
        "- **Extreme events**: Heavy precipitation days\n",
        "\n",
        "### ğŸ“Š About the Data We're Visualizing\n",
        "The patterns below come from our loaded dataset (which could be subsampled or full, depending on what files were available). The spatial and temporal patterns remain representative of European precipitation!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¨ Create Visualization of Precipitation Patterns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 8))  # Increased width for colorbar\n",
        "fig.suptitle('European Precipitation Patterns - Sample Days', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Select 4 interesting time indices (fewer than before)\n",
        "np.random.seed(42)  # For reproducible results\n",
        "time_indices = np.random.choice(len(precip_data.time), 4, replace=False)\n",
        "\n",
        "for i, time_idx in enumerate(time_indices):\n",
        "    row, col = i // 2, i % 2\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Get data for this time step\n",
        "    data_slice = precip_data.isel(time=time_idx)\n",
        "    max_precip = data_slice.rr.max().values\n",
        "    \n",
        "    im = # TASK 1: plot the percipitation data (hint look at imshow)\n",
        "    \n",
        "    # Add informative title\n",
        "    date_str = str(data_slice.time.values)[:10]\n",
        "    ax.set_title(f'{date_str}\\nMax: {max_precip:.1f} mm/day', fontsize=10)\n",
        "    ax.set_xlabel('Longitude (downsampled)')\n",
        "    ax.set_ylabel('Latitude (downsampled)')\n",
        "\n",
        "# Adjust layout to prevent colorbar overlap\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "\n",
        "# Add a colorbar for all subplots - positioned to avoid overlap\n",
        "cbar = plt.colorbar(im, ax=axes, \n",
        "                    pad=0.02, shrink=0.8, location='right')\n",
        "cbar.set_label('Precipitation (mm/day)', fontsize=10)\n",
        "\n",
        "plt.show()\n",
        "del fig, axes, im, data_slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. ğŸ”® Temporal Prediction: Forecasting Tomorrow's Weather & Task 2\n",
        "\n",
        "Now let's build our first self-supervised model! This approach learns to predict tomorrow's precipitation using the past 3 days of data.\n",
        "\n",
        "### ğŸ¤– How it works:\n",
        "1. **Input**: 3 consecutive days of precipitation maps\n",
        "2. **Output**: Precipitation map for day 4 (tomorrow)\n",
        "3. **Learning**: The model discovers temporal patterns without any labels!\n",
        "\n",
        "### ğŸ¯ Why is this useful?\n",
        "- **Weather forecasting**: Predict future precipitation\n",
        "- **Pattern recognition**: Learn seasonal and weather cycles\n",
        "- **Feature extraction**: Create representations for other tasks\n",
        "\n",
        "Let's build the dataset! ğŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ—ï¸ Create Temporal Prediction Dataset \n",
        "print(\"ğŸ”„ Creating temporal prediction dataset...\")\n",
        "\n",
        "print(\"ğŸ“Š Cleaning precipitation data...\")\n",
        "# Handle different data structures\n",
        "if hasattr(precip_data, 'rr'):\n",
        "    precip_data.rr.values = np.nan_to_num(precip_data.rr.values, nan=0.0)\n",
        "    variable_name = 'rr'\n",
        "else:\n",
        "    # If it's the data array directly\n",
        "    precip_data.values = np.nan_to_num(precip_data.values, nan=0.0)\n",
        "    variable_name = precip_data.name if hasattr(precip_data, 'name') else 'data'\n",
        "\n",
        "# Create the temporal prediction dataset\n",
        "print(\"ğŸš€ Creating temporal prediction dataset...\")\n",
        "print(\"   ğŸ“Š Works with both subsampled and full datasets!\")\n",
        "print(\"   ğŸ¯ Dataset size will depend on which data was loaded\")\n",
        "\n",
        "temporal_dataset = EOBSTemporalPredictionDataset(\n",
        "    precipitation_data=precip_data,\n",
        "    sequence_length=3,        # 3 days input\n",
        "    prediction_horizon=1,     # Predict next 1 day\n",
        "    variable_name=variable_name,\n",
        "    spatial_crop_size=(32, 32),  # 32x32 spatial resolution\n",
        "    normalize=True,\n",
        "    log_transform=True\n",
        ")\n",
        "\n",
        "# Split dataset for training and validation\n",
        "train_size = int(0.8 * len(temporal_dataset))\n",
        "val_size = len(temporal_dataset) - train_size\n",
        "\n",
        "# Create workshop-sized subsets\n",
        "workshop_train_size = min(train_size, 1000)\n",
        "workshop_val_size = min(val_size, 250)\n",
        "\n",
        "workshop_train_dataset = # Task 2: create train and test split with sub-sampled sizes\n",
        "workshop_val_dataset = # Task 2: create train and test split with sub-sampled sizes\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(workshop_train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(workshop_val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"âœ… Temporal dataset created!\")\n",
        "print(f\"   ğŸ“Š Total sequences available: {len(temporal_dataset):,}\")\n",
        "\n",
        "# Determine if we're using subsampled or full data\n",
        "total_sequences = len(temporal_dataset)\n",
        "if total_sequences < 5000:\n",
        "    print(f\"   ğŸš€ Using subsampled data (fast processing!)\")\n",
        "elif total_sequences > 20000:\n",
        "    print(f\"   ğŸ“š Using full dataset (complete data)\")\n",
        "else:\n",
        "    print(f\"   ğŸ“Š Using intermediate dataset size\")\n",
        "\n",
        "print(f\"   ğŸƒ Workshop training samples: {len(workshop_train_dataset):,}\")\n",
        "print(f\"   âœ… Workshop validation samples: {len(workshop_val_dataset):,}\")\n",
        "print(f\"   ğŸ¯ Input: 3 days Ã— 32Ã—32 pixels\")\n",
        "print(f\"   ğŸ¯ Output: 1 day Ã— 32Ã—32 pixels\")\n",
        "print(f\"   ğŸ’¾ Batch size: 8\")\n",
        "\n",
        "# Clear temporary variables\n",
        "del train_dataset, val_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ” Explore the Temporal Dataset\n",
        "print(\"ğŸ“Š Let's examine a sample from our temporal dataset...\")\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "inputs, targets = sample_batch\n",
        "\n",
        "print(f\"âœ… Sample batch loaded:\")\n",
        "print(f\"   ğŸ”¢ Input shape: {inputs.shape}  # (batch_size, sequence_days, height, width)\")\n",
        "print(f\"   ğŸ”¢ Target shape: {targets.shape}  # (batch_size, prediction_days, height, width)\")\n",
        "\n",
        "# Visualize a sample sequence\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "fig.suptitle('ğŸ”® Temporal Prediction: Learning from Past to Predict Future', fontsize=16, fontweight='bold')\n",
        "\n",
        "sample_idx = 0\n",
        "input_seq = inputs[sample_idx]  # (3, 32, 32)\n",
        "target_day = targets[sample_idx, 0]  # (32, 32)\n",
        "\n",
        "# Plot input sequence (3 days)\n",
        "for i in range(3):\n",
        "    ax = axes[i]\n",
        "    im = ax.imshow(input_seq[i].squeeze(), cmap='Blues', vmin=0, vmax=3)\n",
        "    ax.set_title(f'Day {i-2}\\n(Input)', fontsize=12)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Plot target (prediction day)\n",
        "ax = axes[3]\n",
        "im = ax.imshow(target_day.squeeze(), cmap='Blues', vmin=0, vmax=3)\n",
        "ax.set_title('Day +1\\n(Target)', fontweight='bold', color='red', fontsize=12)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Understanding the Task:\")\n",
        "print(\"   - Model sees 3 consecutive days of precipitation\")\n",
        "print(\"   - Goal: Predict the precipitation pattern for day 4\")\n",
        "print(\"   - This is like teaching the model to be a weather forecaster!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ Train Temporal Prediction Model\n",
        "print(\"ğŸ¤– Building our temporal prediction model...\")\n",
        "\n",
        "# Create temporal prediction model\n",
        "temporal_model = TemporalPredictionModel(\n",
        "    input_channels=1,        # Single channel (precipitation)\n",
        "    hidden_channels=8,       \n",
        "    num_layers=4,           \n",
        "    sequence_length=3,       # number of past/input days\n",
        "    prediction_horizon=1,    # Predict 1 day ahead\n",
        "    spatial_size=(32, 32)    \n",
        ")\n",
        "\n",
        "# Wrap in PyTorch Lightning for clean training\n",
        "temporal_lightning_model = TemporalPredictionLightningModule(\n",
        "    model=temporal_model,\n",
        "    learning_rate=2e-3,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Setup training callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        mode='min'\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        dirpath='../src/runs/temporal_prediction_lightning/',\n",
        "        filename='best-checkpoint',\n",
        "        save_top_k=1,\n",
        "        mode='min'\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create the trainer\n",
        "temporal_trainer = L.Trainer(\n",
        "    max_epochs=6,\n",
        "    callbacks=callbacks,\n",
        "    log_every_n_steps=10,\n",
        "    precision='16-mixed',    \n",
        "    enable_checkpointing=True,\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=False,\n",
        "    accelerator='auto',\n",
        "    devices='auto'\n",
        ")\n",
        "\n",
        "print(\"ğŸ¯ Training Configuration:\")\n",
        "print(f\"   - Model: ConvLSTM with {sum(p.numel() for p in temporal_model.parameters()):,} parameters\")\n",
        "print(f\"   - Training samples: {len(workshop_train_dataset):,}\")\n",
        "print(f\"   - Max epochs: 6 (with early stopping)\")\n",
        "print(f\"   - Sequence length: 3 days\")\n",
        "print(f\"   - Learning rate: 2e-3\")\n",
        "print(f\"   - Precision: 16-bit mixed precision\")\n",
        "print(f\"   - Spatial size: 32x32 (reduced for speed)\")\n",
        "\n",
        "# Start training!\n",
        "print(\"\\nğŸš€ Starting temporal prediction training...\")\n",
        "print(\"âš¡ This should take about 2-3 minutes!\")\n",
        "\n",
        "# Task 3: fit the model\n",
        "\n",
        "print(f\"\\nğŸ‰ Training completed successfully!\")\n",
        "print(f\"   âœ… Best validation loss: {temporal_trainer.checkpoint_callback.best_model_score:.6f}\")\n",
        "print(f\"   ğŸ“ Model saved at: {temporal_trainer.checkpoint_callback.best_model_path}\")\n",
        "\n",
        "# Save the best model path before deleting trainer\n",
        "best_model_path_temporal = temporal_trainer.checkpoint_callback.best_model_path\n",
        "\n",
        "del temporal_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ’¾ Save Our Trained Model\n",
        "print(\"ğŸ’¾ Saving the trained temporal prediction model...\")\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('../src/models', exist_ok=True)\n",
        "\n",
        "# Load and save the best model\n",
        "best_model = TemporalPredictionLightningModule.load_from_checkpoint(\n",
        "    best_model_path_temporal,\n",
        "    model=temporal_model\n",
        ")\n",
        "\n",
        "# Save for future use\n",
        "torch.save(\n",
        "    best_model.model.state_dict(),\n",
        "    '../src/models/temporal_prediction_model.pth'\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model saved successfully!\")\n",
        "print(f\"   ğŸ“ Location: ../src/models/temporal_prediction_model.pth\")\n",
        "print(f\"   ğŸ’¡ You can load this model later for inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. ğŸ­ Masked Modeling: Filling in Missing Pieces\n",
        "\n",
        "Now let's build our second self-supervised model! This approach learns to reconstruct missing parts of precipitation maps.\n",
        "\n",
        "### ğŸ§© How it works:\n",
        "1. **Input**: Precipitation map with random patches masked out\n",
        "2. **Output**: Complete precipitation map (original)\n",
        "3. **Learning**: The model learns spatial relationships and patterns\n",
        "\n",
        "### ğŸ¯ Why is this useful?\n",
        "- **Data imputation**: Fill missing data in weather observations\n",
        "- **Anomaly detection**: Identify unusual patterns\n",
        "- **Data compression**: Learn compact representations\n",
        "- **Quality control**: Detect sensor errors\n",
        "\n",
        "This is like teaching the model to be a \"weather detective\" - filling in missing clues! ğŸ”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ—ï¸ Create Masked Modeling Dataset\n",
        "print(\"ğŸ”„ Creating masked modeling dataset...\")\n",
        "\n",
        "if hasattr(precip_data, 'rr'):\n",
        "    variable_name = 'rr'\n",
        "else:\n",
        "    variable_name = precip_data.name if hasattr(precip_data, 'name') else 'data'\n",
        "\n",
        "print(\"ğŸ­ Creating masked modeling dataset...\")\n",
        "print(\"   ğŸ“Š Also works with both subsampled and full datasets!\")\n",
        "print(\"   ğŸ¯ Uses the same data that was loaded earlier\")\n",
        "\n",
        "# Create the masked modeling dataset with reduced memory footprint\n",
        "masked_dataset = EOBSMaskedModelingDataset(\n",
        "    precipitation_data=precip_data,\n",
        "    variable_name=variable_name,\n",
        "    spatial_size=(48, 48),  \n",
        "    mask_ratio=0.25,           # Mask 25% of each image\n",
        "    mask_strategy='random_patches',  # Use random patch masking\n",
        "    patch_size=6,             \n",
        "    normalize=True,\n",
        "    log_transform=True,\n",
        "    temporal_context=1         # Single time step\n",
        ")\n",
        "\n",
        "# Split dataset for training and validation\n",
        "train_size = int(0.8 * len(masked_dataset))\n",
        "val_size = len(masked_dataset) - train_size\n",
        "\n",
        "workshop_train_size_masked = min(train_size, 1000)  # Reduced from 2000 to 1000\n",
        "workshop_val_size_masked = min(val_size, 250)       # Reduced from 500 to 250\n",
        "\n",
        "# Create data splits\n",
        "train_dataset_masked, val_dataset_masked = torch.utils.data.random_split(\n",
        "    masked_dataset, [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Create workshop-sized subsets\n",
        "workshop_train_dataset_masked = torch.utils.data.Subset(train_dataset_masked, range(workshop_train_size_masked))\n",
        "workshop_val_dataset_masked = torch.utils.data.Subset(val_dataset_masked, range(workshop_val_size_masked))\n",
        "\n",
        "# MEMORY OPTIMIZATION: Reduce batch size for better memory management\n",
        "train_loader_masked = DataLoader(workshop_train_dataset_masked, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader_masked = DataLoader(workshop_val_dataset_masked, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"âœ… Masked modeling dataset created!\")\n",
        "print(f\"   ğŸ“Š Total samples available: {len(masked_dataset):,}\")\n",
        "\n",
        "# Determine if we're using subsampled or full data\n",
        "total_samples = len(masked_dataset)\n",
        "if total_samples < 5000:\n",
        "    print(f\"   ğŸš€ Using subsampled data (fast processing!)\")\n",
        "elif total_samples > 20000:\n",
        "    print(f\"   ğŸ“š Using full dataset (complete data)\")\n",
        "else:\n",
        "    print(f\"   ğŸ“Š Using intermediate dataset size\")\n",
        "\n",
        "print(f\"   ğŸƒ Workshop training samples: {len(workshop_train_dataset_masked):,}\")\n",
        "print(f\"   âœ… Workshop validation samples: {len(workshop_val_dataset_masked):,}\")\n",
        "print(f\"   ğŸ¯ Task: Reconstruct 25% masked areas\")\n",
        "print(f\"   ğŸ§© Patch size: 6Ã—6 pixels\")\n",
        "print(f\"   ğŸ’¾ Batch size: 8\")\n",
        "\n",
        "del train_dataset_masked, val_dataset_masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ” Explore the Masked Modeling Dataset\n",
        "print(\"ğŸ“Š Let's examine a sample from our masked modeling dataset...\")\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch_masked = next(iter(train_loader_masked))\n",
        "masked_inputs, targets, masks = sample_batch_masked\n",
        "\n",
        "print(f\"âœ… Sample batch loaded:\")\n",
        "print(f\"   ğŸ”¢ Masked input shape: {masked_inputs.shape}\")\n",
        "print(f\"   ğŸ”¢ Target shape: {targets.shape}\")\n",
        "print(f\"   ğŸ”¢ Mask shape: {masks.shape}\")\n",
        "\n",
        "# Visualize the masking process\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('ğŸ­ Masked Modeling: Teaching the Model to Fill Missing Pieces', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Store the image mappables for colorbars\n",
        "im_precip = None\n",
        "im_mask = None\n",
        "\n",
        "for i in range(2):\n",
        "    # Original (complete precipitation map)\n",
        "    ax = axes[i, 0]\n",
        "    original = targets[i, 0]  # (H, W)\n",
        "    im_precip = ax.imshow(original, cmap='Blues', vmin=0, vmax=3)\n",
        "    ax.set_title(f'Original {i+1}\\n(Complete)', fontsize=12)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Masked (input with missing patches)\n",
        "    ax = axes[i, 1]\n",
        "    masked = masked_inputs[i, 0]  # (H, W)\n",
        "    im_precip = ax.imshow(masked, cmap='Blues', vmin=0, vmax=3)\n",
        "    ax.set_title(f'Masked {i+1}\\n(Input)', fontsize=12)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Mask (shows which areas are masked)\n",
        "    ax = axes[i, 2]\n",
        "    mask = masks[i]  # (H, W)\n",
        "    im_mask = ax.imshow(mask, cmap='Reds', vmin=0, vmax=1)\n",
        "    ax.set_title(f'Mask {i+1}\\n(Missing areas)', fontsize=12)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust layout to prevent colorbar overlap\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "\n",
        "# Add colorbars for precipitation and mask data\n",
        "if im_precip is not None:\n",
        "    cbar_precip = plt.colorbar(im_precip, ax=axes[:, :2], \n",
        "                                pad=0.02, shrink=0.6, location='right')\n",
        "    cbar_precip.set_label('Precipitation (mm/day)', fontsize=10)\n",
        "\n",
        "if im_mask is not None:\n",
        "    cbar_mask = plt.colorbar(im_mask, ax=axes[:, 2], \n",
        "                            pad=0.02, shrink=0.6, location='right')\n",
        "    cbar_mask.set_label('Mask (0=masked, 1=visible)', fontsize=10)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print masking statistics\n",
        "mask_ratio = masks.float().mean()\n",
        "print(f\"\\nğŸ¯ Understanding the Task:\")\n",
        "print(f\"   - Model sees precipitation maps with missing patches (black areas)\")\n",
        "print(f\"   - Goal: Reconstruct the complete precipitation pattern\")\n",
        "print(f\"   - Red areas in mask show which regions need to be filled\")\n",
        "print(f\"   - Average mask ratio: {mask_ratio:.1%} ({mask_ratio * 48 * 48:.0f} pixels)\")\n",
        "print(f\"   - This teaches the model spatial relationships in weather data!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸš€ Train Masked Modeling Model\n",
        "\n",
        "Now let's train our second model! This U-Net architecture will learn to reconstruct missing precipitation patterns.\n",
        "\n",
        "### ğŸ—ï¸ Model Architecture\n",
        "- **U-Net**: Encoder-decoder with skip connections\n",
        "- **Task**: Fill in missing 25% of precipitation data\n",
        "- **Learning**: Spatial relationships and texture patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¤– Build and Train Masked Modeling Model\n",
        "print(\"ğŸ¤– Building our masked modeling model...\")\n",
        "\n",
        "# Create masked modeling model\n",
        "masked_model = MaskedModelingModel(\n",
        "    input_channels=1,      # Single channel (precipitation)\n",
        "    base_channels=16,      # 16 base channels\n",
        "    num_levels=2,          # 2 levels in U-Net\n",
        "    temporal_context=1     # Single time step\n",
        ")\n",
        "\n",
        "# Wrap in PyTorch Lightning for clean training\n",
        "masked_lightning_model = MaskedModelingLightningModule(\n",
        "    model=masked_model,\n",
        "    learning_rate=1e-3,    # Stable learning rate\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Setup training callbacks\n",
        "callbacks_masked = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        verbose=True,\n",
        "        mode='min'\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        dirpath='../src/runs/masked_modeling_lightning/',\n",
        "        filename='best-checkpoint',\n",
        "        save_top_k=1,\n",
        "        mode='min'\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create the trainer\n",
        "masked_trainer = L.Trainer(\n",
        "    max_epochs=6,\n",
        "    callbacks=callbacks_masked,\n",
        "    log_every_n_steps=20,\n",
        "    precision='16-mixed',\n",
        "    gradient_clip_val=0.5,\n",
        "    enable_checkpointing=True,\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=False,\n",
        "    accelerator='auto',\n",
        "    devices='auto'\n",
        ")\n",
        "\n",
        "print(\"ğŸ¯ Training Configuration:\")\n",
        "print(f\"   - Model: U-Net with {sum(p.numel() for p in masked_model.parameters()):,} parameters\")\n",
        "print(f\"   - Training samples: {len(workshop_train_dataset_masked):,}\")\n",
        "print(f\"   - Max epochs: 6 (with early stopping)\")\n",
        "print(f\"   - Learning rate: 1e-3\")\n",
        "\n",
        "# Start training!\n",
        "print(\"\\nğŸš€ Starting masked modeling training...\")\n",
        "print(\"â˜• This will take about 10-15 minutes. Time for another coffee!\")\n",
        "\n",
        "masked_trainer.fit(\n",
        "    masked_lightning_model,\n",
        "    train_dataloaders=train_loader_masked,\n",
        "    val_dataloaders=val_loader_masked\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ‰ Training completed successfully!\")\n",
        "print(f\"   âœ… Best validation loss: {masked_trainer.checkpoint_callback.best_model_score:.6f}\")\n",
        "print(f\"   ğŸ“ Model saved at: {masked_trainer.checkpoint_callback.best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š Evaluate Temporal Prediction Model\n",
        "print(\"ğŸ”® Evaluating temporal prediction model...\")\n",
        "\n",
        "temporal_lightning_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Get a validation batch\n",
        "    val_batch = next(iter(val_loader))\n",
        "    # Task 4: write model inference code for the temporal model (temporal_lightning_model)\n",
        "    \n",
        "    predictions = \n",
        "    inputs = \n",
        "    targets = \n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = torch.nn.functional.mse_loss(predictions, targets)\n",
        "    mae = torch.nn.functional.l1_loss(predictions, targets)\n",
        "    \n",
        "    print(f\"âœ… Temporal Prediction Results:\")\n",
        "    print(f\"   ğŸ“Š MSE: {mse:.6f}\")\n",
        "    print(f\"   ğŸ“Š MAE: {mae:.6f}\")\n",
        "    \n",
        "    # Visualize predictions\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('ğŸ”® Temporal Prediction Results: Past â†’ Future', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Store the image mappable for colorbar\n",
        "    im_precip = None\n",
        "    \n",
        "    for i in range(2):\n",
        "        # Input (last day of sequence)\n",
        "        ax = axes[i, 0]\n",
        "        last_input = inputs_cpu[i, -1]  # Last day of input sequence\n",
        "        im_precip = ax.imshow(last_input.squeeze(), cmap='Blues', vmin=0, vmax=3)\n",
        "        ax.set_title(f'Input Day {i+1}\\n(Day -1)', fontsize=12)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Prediction\n",
        "        ax = axes[i, 1]\n",
        "        pred = predictions_cpu[i, 0]  # First (only) prediction\n",
        "        im_precip = ax.imshow(pred.squeeze(), cmap='Blues', vmin=0, vmax=3)\n",
        "        ax.set_title(f'Prediction {i+1}\\n(Day +1)', fontsize=12)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Ground truth\n",
        "        ax = axes[i, 2]\n",
        "        target = targets_cpu[i, 0]  # First (only) target\n",
        "        im_precip = ax.imshow(target.squeeze(), cmap='Blues', vmin=0, vmax=3)\n",
        "        ax.set_title(f'Ground Truth {i+1}\\n(Day +1)', fontsize=12)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    # Adjust layout to prevent colorbar overlap\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(right=0.85)\n",
        "    \n",
        "    # Add colorbar for precipitation data\n",
        "    if im_precip is not None:\n",
        "        cbar = plt.colorbar(im_precip, ax=axes, \n",
        "                           pad=0.02, shrink=0.8, location='right')\n",
        "        cbar.set_label('Precipitation (mm/day)', fontsize=10)\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    print(\"ğŸ’¡ Observations:\")\n",
        "    print(\"   - Model learns to predict general precipitation patterns\")\n",
        "    print(\"   - Spatial structure is preserved in predictions\")\n",
        "    print(\"   - Some details may be smoothed due to inherent prediction uncertainty\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ­ Evaluate Masked Modeling\n",
        "print(\"ğŸ”® Evaluating masked modeling reconstruction...\")\n",
        "\n",
        "masked_lightning_model.eval()\n",
        "masked_lightning_model = masked_lightning_model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Get a validation batch\n",
        "    val_batch = next(iter(val_loader_masked))\n",
        "    # Task 5: write model inference code for the masked model (masked_lightning_model)\n",
        "    \n",
        "    predictions = \n",
        "    masked_inputs = \n",
        "    targets = \n",
        "    \n",
        "    # Calculate metrics only on masked regions\n",
        "    if masks.sum() > 0:\n",
        "        # Expand mask to match predictions/targets shape [8, 1, 48, 48]\n",
        "        mask_expanded = masks.bool().unsqueeze(1)  # [8, 48, 48] -> [8, 1, 48, 48]\n",
        "        \n",
        "        # Extract only the masked regions for computing metrics\n",
        "        masked_predictions = predictions[mask_expanded]\n",
        "        masked_targets = targets[mask_expanded]\n",
        "        \n",
        "        masked_mse = torch.nn.functional.mse_loss(masked_predictions, masked_targets)\n",
        "        masked_mae = torch.nn.functional.l1_loss(masked_predictions, masked_targets)\n",
        "        \n",
        "        print(f\"âœ… Masked Modeling Results:\")\n",
        "        print(f\"   ğŸ“Š MSE (masked regions): {masked_mse:.6f}\")\n",
        "        print(f\"   ğŸ“Š MAE (masked regions): {masked_mae:.6f}\")\n",
        "        print(f\"   ğŸ“Š Masked pixels: {masks.sum()}/{masks.numel()} ({masks.float().mean():.2%})\")\n",
        "    \n",
        "    # Visualize reconstructions\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    fig.suptitle('ğŸ­ Masked Modeling Reconstruction Results', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Store the image mappables for colorbars\n",
        "    im_precip = None\n",
        "    im_mask = None\n",
        "    \n",
        "    for i in range(2):\n",
        "        # Original\n",
        "        ax = axes[i, 0]\n",
        "        original = targets_cpu[i, 0]\n",
        "        im_precip = ax.imshow(original, cmap='Blues', vmin=0, vmax=3)\n",
        "        ax.set_title(f'Original {i+1}', fontsize=12)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Masked input\n",
        "        ax = axes[i, 1]\n",
        "        masked_input = masked_inputs_cpu[i, 0]\n",
        "        im_precip = ax.imshow(masked_input, cmap='Blues', vmin=0, vmax=3)\n",
        "        ax.set_title(f'Masked Input {i+1}', fontsize=12)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Prediction\n",
        "        ax = axes[i, 2]\n",
        "        pred = predictions_cpu[i, 0]\n",
        "        im_precip = ax.imshow(pred, cmap='Blues', vmin=0, vmax=3)\n",
        "        ax.set_title(f'Reconstruction {i+1}', fontsize=12)\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Mask visualization\n",
        "        ax = axes[i, 3]\n",
        "        mask_vis = masks[i]  # masks_cpu has shape [batch, height, width]\n",
        "        im_mask = ax.imshow(mask_vis, cmap='Reds', vmin=0, vmax=1)\n",
        "        ax.set_title(f'Mask {i+1}', fontsize=12)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    # Adjust layout to prevent colorbar overlap\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(right=0.85)\n",
        "    \n",
        "    # Add colorbars for precipitation and mask data\n",
        "    if im_precip is not None:\n",
        "        cbar_precip = plt.colorbar(im_precip, ax=axes[:, :3], \n",
        "                                  pad=0.02, shrink=0.6, location='right')\n",
        "        cbar_precip.set_label('Precipitation (mm/day)', fontsize=10)\n",
        "    \n",
        "    if im_mask is not None:\n",
        "        cbar_mask = plt.colorbar(im_mask, ax=axes[:, 3], \n",
        "                                pad=0.02, shrink=0.6, location='right')\n",
        "        cbar_mask.set_label('Mask (1=masked, 0=visible)', fontsize=10)\n",
        "    \n",
        "    print(\"ğŸ’¡ Observations:\")\n",
        "    print(\"   - Model learns to reconstruct masked regions\")\n",
        "    print(\"   - Spatial coherence is maintained in reconstructions\")\n",
        "    print(\"   - Model captures patterns from visible context\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Potential Improvements / Next Steps\n",
        "1. **Extend to Multiple Variables**: Include temperature, pressure, humidity\n",
        "2. **Multi-Scale Modeling**: Work with different spatial resolutions\n",
        "3. **Physics-Informed Constraints**: Incorporate conservation laws\n",
        "4. **Transfer Learning**: Fine-tune on specific regions or tasks\n",
        "5. **Hyperparameter Tuning**: Use Lightning's built-in HPO tools\n",
        "7. **Evaluation on Downstream Tasks**: Test learned representations on classification/regression tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Downstream Tasks: Classifier on SSL Embeddings\n",
        "\n",
        "Now let's demonstrate how to use the learned self-supervised embeddings for downstream tasks. We'll create a classifier that uses the embeddings from our trained models to classify precipitation patterns.\n",
        "\n",
        "### ğŸ¯ Task: High vs Low Precipitation Classification\n",
        "We'll extract embeddings from our trained models and use them to classify weather patterns into high and low precipitation categories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import additional libraries for downstream tasks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "print(\"ğŸ”§ Additional libraries loaded for downstream tasks\")\n",
        "\n",
        "# Create a simple classifier that uses SSL embeddings\n",
        "class EmbeddingClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = # Task 6: create a simple classifier with two layers, activation function and dropout\n",
        "    \n",
        "    def forward(self, embeddings):\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "print(\"âœ… Embedding classifier defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained models and extract embeddings\n",
        "print(\"ğŸ”® Extracting embeddings from trained models...\")\n",
        "\n",
        "# Extract embeddings from all samples in the validation dataset\n",
        "temporal_lightning_model.eval()\n",
        "all_embeddings = []\n",
        "all_targets = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"   - Processing all validation samples...\")\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        # Extract embeddings by forward pass through temporal model\n",
        "        temporal_features = temporal_lightning_model(inputs)\n",
        "        embeddings = temporal_features.view(temporal_features.size(0), -1)  # Flatten\n",
        "        \n",
        "        # Create labels based on precipitation intensity\n",
        "        precip_mean = targets.mean(dim=[1, 2, 3, 4])  # Shape: [batch_size]\n",
        "        threshold = precip_mean.median()\n",
        "        labels = (precip_mean > threshold).long()\n",
        "        \n",
        "        all_embeddings.append(embeddings.cpu())\n",
        "        all_targets.append(targets.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "# Concatenate all embeddings and labels\n",
        "embeddings = torch.cat(all_embeddings, dim=0)\n",
        "targets = torch.cat(all_targets, dim=0)\n",
        "labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "print(f\"   - Total embeddings shape: {embeddings.shape}\")\n",
        "print(f\"   - Total labels shape: {labels.shape}\")\n",
        "print(f\"   - Label range: {labels.min().item()}-{labels.max().item()}\")\n",
        "\n",
        "# Fix the bincount issue by ensuring labels are 1D and non-negative\n",
        "labels_1d = labels.flatten()\n",
        "if labels_1d.min() < 0:\n",
        "    labels_1d = labels_1d - labels_1d.min()\n",
        "print(f\"   - Label distribution: {torch.bincount(labels_1d)}\")\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    embeddings.numpy(), labels.numpy(), \n",
        "    test_size=0.3, random_state=42,\n",
        ")\n",
        "\n",
        "print(f\"   - Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "print(\"âœ… Embeddings extracted and prepared for classification!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train classifiers on SSL embeddings\n",
        "print(\"ğŸ¯ Training classifiers on SSL embeddings...\")\n",
        "\n",
        "# 1. Random Forest Classifier\n",
        "print(\"   - Training Random Forest...\")\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "rf_pred = rf_classifier.predict(X_test)\n",
        "rf_accuracy = (rf_pred == y_test).mean()\n",
        "\n",
        "# 2. Logistic Regression\n",
        "print(\"   - Training Logistic Regression...\")\n",
        "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "lr_pred = lr_classifier.predict(X_test)\n",
        "lr_accuracy = (lr_pred == y_test).mean()\n",
        "\n",
        "# 3. Neural Network Classifier\n",
        "print(\"   - Training Neural Network...\")\n",
        "nn_classifier = EmbeddingClassifier(embeddings.shape[1], 2).to(device)\n",
        "nn_optimizer = torch.optim.Adam(nn_classifier.parameters(), lr=0.001)\n",
        "nn_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "# Train neural network\n",
        "nn_classifier.train()\n",
        "for epoch in range(100):\n",
        "    nn_optimizer.zero_grad()\n",
        "    outputs = nn_classifier(X_train_tensor)\n",
        "    loss = nn_criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    nn_optimizer.step()\n",
        "\n",
        "# Evaluate neural network\n",
        "nn_classifier.eval()\n",
        "with torch.no_grad():\n",
        "    nn_outputs = nn_classifier(X_test_tensor)\n",
        "    nn_pred = torch.argmax(nn_outputs, dim=1).cpu().numpy()\n",
        "nn_accuracy = (nn_pred == y_test).mean()\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nğŸ“Š Classification Results:\")\n",
        "print(f\"   - Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"   - Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"   - Neural Network Accuracy: {nn_accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification report for best model\n",
        "best_model = \"Random Forest\" if rf_accuracy >= max(lr_accuracy, nn_accuracy) else \"Logistic Regression\" if lr_accuracy >= nn_accuracy else \"Neural Network\"\n",
        "best_pred = rf_pred if rf_accuracy >= max(lr_accuracy, nn_accuracy) else lr_pred if lr_accuracy >= nn_accuracy else nn_pred\n",
        "\n",
        "print(f\"\\nğŸ† Best Model: {best_model}\")\n",
        "print(\"\\nğŸ“ˆ Detailed Classification Report:\")\n",
        "print(classification_report(y_test, best_pred, target_names=['Low Precipitation', 'High Precipitation']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, best_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Low Precipitation', 'High Precipitation'],\n",
        "            yticklabels=['Low Precipitation', 'High Precipitation'])\n",
        "plt.title(f'Confusion Matrix - {best_model}')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Classification on SSL embeddings completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Clustering and Retrieval with SSL Embeddings\n",
        "\n",
        "Now let's explore the learned embeddings through clustering and implement a retrieval system to find similar weather patterns.\n",
        "\n",
        "### ğŸ¯ Goals:\n",
        "1. **Clustering**: Discover natural groups in the learned embedding space\n",
        "2. **Retrieval**: Find similar weather patterns given a query\n",
        "3. **Visualization**: Understand the embedding space structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clustering analysis on UNet bottleneck embeddings\n",
        "\n",
        "print(\"ğŸ¯ Performing clustering analysis on UNet bottleneck embeddings...\")\n",
        "\n",
        "# Extract bottleneck embeddings from the UNet masked modeling model\n",
        "print(\"   - Extracting bottleneck embeddings from UNet...\")\n",
        "masked_lightning_model.eval()\n",
        "all_bottleneck_embeddings = []\n",
        "all_bottleneck_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        # Use only the first frame for masked modeling (since it expects single images)\n",
        "        input_images = inputs[:, 0]  # Shape: [batch_size, channels, height, width]\n",
        "        \n",
        "        # Extract bottleneck embeddings using our new method\n",
        "        bottleneck_embeddings = masked_lightning_model.model.extract_bottleneck_embeddings(input_images)\n",
        "        \n",
        "        # Create labels based on precipitation intensity\n",
        "        precip_mean = targets.mean(dim=[1, 2, 3, 4])  # Shape: [batch_size]\n",
        "        threshold = precip_mean.median()\n",
        "        labels = (precip_mean > threshold).long()\n",
        "        \n",
        "        all_bottleneck_embeddings.append(bottleneck_embeddings.cpu())\n",
        "        all_bottleneck_labels.append(labels.cpu())\n",
        "\n",
        "# Concatenate all bottleneck embeddings and labels\n",
        "bottleneck_embeddings = torch.cat(all_bottleneck_embeddings, dim=0)\n",
        "bottleneck_labels = torch.cat(all_bottleneck_labels, dim=0)\n",
        "\n",
        "print(f\"   - Bottleneck embeddings shape: {bottleneck_embeddings.shape}\")\n",
        "print(f\"   - Bottleneck labels shape: {bottleneck_labels.shape}\")\n",
        "\n",
        "# Convert to numpy for clustering\n",
        "embeddings_np = bottleneck_embeddings.numpy()\n",
        "labels_np = bottleneck_labels.numpy()\n",
        "\n",
        "# Apply K-means clustering\n",
        "print(\"   - Applying K-means clustering...\")\n",
        "n_clusters = 4  # Try 4 weather pattern clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(embeddings_np)\n",
        "\n",
        "# Calculate cluster statistic\n",
        "cluster_counts = np.bincount(cluster_labels)\n",
        "print(f\"   - Cluster distribution: {cluster_counts}\")\n",
        "\n",
        "# Analyze clusters by precipitation labels\n",
        "print(\"\\nğŸ” Cluster Analysis by Precipitation Level:\")\n",
        "for cluster_id in range(n_clusters):\n",
        "    cluster_mask = cluster_labels == cluster_id\n",
        "    cluster_precip_labels = labels_np[cluster_mask]\n",
        "    low_precip_count = (cluster_precip_labels == 0).sum()\n",
        "    high_precip_count = (cluster_precip_labels == 1).sum()\n",
        "    \n",
        "    print(f\"   - Cluster {cluster_id}: {cluster_counts[cluster_id]} samples\")\n",
        "    print(f\"     â€¢ Low precipitation: {low_precip_count} ({low_precip_count/cluster_counts[cluster_id]*100:.1f}%)\")\n",
        "    print(f\"     â€¢ High precipitation: {high_precip_count} ({high_precip_count/cluster_counts[cluster_id]*100:.1f}%)\")\n",
        "\n",
        "# Reduce dimensionality for visualization\n",
        "print(\"\\nğŸ“Š Preparing visualization...\")\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "embeddings_2d = pca.fit_transform(embeddings_np)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f\"   - PCA explained variance: {explained_variance[0]:.3f}, {explained_variance[1]:.3f}\")\n",
        "print(f\"   - Total explained variance: {explained_variance.sum():.3f}\")\n",
        "\n",
        "# Plot clustering results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Clusters\n",
        "ax1 = axes[0]\n",
        "scatter = ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                        c=cluster_labels, cmap='viridis', alpha=0.7)\n",
        "ax1.set_title('UNet Bottleneck Embeddings: K-means Clusters')\n",
        "ax1.set_xlabel(f'PC1 ({explained_variance[0]:.3f} variance)')\n",
        "ax1.set_ylabel(f'PC2 ({explained_variance[1]:.3f} variance)')\n",
        "cbar = plt.colorbar(scatter, ax=ax1, label='Cluster ID', ticks=range(n_clusters))\n",
        "cbar.set_ticklabels([f'Cluster {i}' for i in range(n_clusters)])\n",
        "\n",
        "# Plot 2: Precipitation levels\n",
        "ax2 = axes[1]\n",
        "colors = ['blue', 'red']\n",
        "labels_names = ['Low Precipitation', 'High Precipitation']\n",
        "for i, (color, label_name) in enumerate(zip(colors, labels_names)):\n",
        "    mask = labels_np == i\n",
        "    ax2.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
        "                c=color, alpha=0.7, label=label_name)\n",
        "ax2.set_title('UNet Bottleneck Embeddings: Precipitation Levels')\n",
        "ax2.set_xlabel(f'PC1 ({explained_variance[0]:.3f} variance)')\n",
        "ax2.set_ylabel(f'PC2 ({explained_variance[1]:.3f} variance)')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Clustering analysis completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¨ Understanding the Challenge: Why Self-Supervised Learning is Hard for Weather\n",
        "print(\"\\nğŸ¨ Understanding the Challenge: Why Self-Supervised Learning is Hard for Weather\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if we have trained models available\n",
        "if 'temporal_lightning_model' in locals() and 'masked_lightning_model' in locals():\n",
        "    print(\"âœ… Both models trained - let's understand the challenges they face...\")\n",
        "    \n",
        "    # Analyze the inherent difficulties\n",
        "    print(\"\\nğŸ¤” Why These Tasks Are Genuinely Difficult:\")\n",
        "    print(\"\\n1ï¸âƒ£ **Temporal Prediction Challenge:**\")\n",
        "    print(\"   ğŸŒ§ï¸ Weather is inherently chaotic - small changes can lead to big differences\")\n",
        "    print(\"   ğŸ“Š Precipitation is highly non-linear and depends on many factors\")\n",
        "    print(\"   ğŸ—“ï¸ 7 days of history often isn't enough to predict tomorrow's rain\")\n",
        "    print(\"   ğŸŒ¡ï¸ Missing atmospheric conditions (temperature, pressure, humidity)\")\n",
        "    print(\"   ğŸ’­ Think about it: Can you predict if it'll rain tomorrow just from\")\n",
        "    print(\"      looking at the past week's precipitation patterns?\")\n",
        "    \n",
        "    print(\"\\n2ï¸âƒ£ **Masked Modeling Challenge:**\")\n",
        "    print(\"   â˜ï¸ Weather systems are complex - rain doesn't follow simple spatial rules\")\n",
        "    print(\"   ğŸ¯ Guessing rainfall in the middle of a cloud is surprisingly hard!\")\n",
        "    print(\"   ğŸŒªï¸ Weather fronts and systems can have sharp boundaries\")\n",
        "    print(\"   ğŸ“ Local geography and orography affect precipitation patterns\")\n",
        "    print(\"   ğŸ’­ Think about it: If you see rain around a region, how much rain\")\n",
        "    print(\"      should be in the center? It depends on so many factors!\")\n",
        "    \n",
        "    print(\"\\nğŸš€ Moving Forward - Better Approaches:\")\n",
        "    print(\"   â€¢ Include multiple weather variables (temperature, pressure, wind)\")\n",
        "    print(\"   â€¢ Use physics-informed neural networks\")\n",
        "    print(\"   â€¢ Incorporate atmospheric dynamics equations\")\n",
        "    print(\"   â€¢ Combine with traditional weather models\")\n",
        "    print(\"   â€¢ Use larger spatio-temporal windows and better spatial resolutions\")\n",
        "    \n",
        "    print(\"\\nğŸ’¡ Key Takeaway:\")\n",
        "    print(\"   Self-supervised learning is powerful, but weather is complex!\")\n",
        "    print(\"   This workshop shows both the potential and the challenges.\")\n",
        "    print(\"   The 'failures' teach us as much as the successes! ğŸ“\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸ Models not fully trained - run the training cells first!\")\n",
        "    print(\"   These examples show what you'll see after training.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ‰ Workshop Complete - Great Learning Experience!\")\n",
        "print(\"=\"*50)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

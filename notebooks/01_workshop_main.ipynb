{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ Introduction to AI Workshop\n",
        "## Satellite Imagery Classification: CNN vs Vision Transformers\n",
        "\n",
        "**Duration:** 90 minutes  \n",
        "**Dataset:** EuroSAT (Satellite Land Use Classification)  \n",
        "**Goal:** Compare CNN and Vision Transformer architectures  \n",
        "\n",
        "---\n",
        "\n",
        "### üìö What You'll Learn\n",
        "- CNN architecture and feature visualization\n",
        "- Vision Transformers and attention mechanism\n",
        "- HuggingFace integration\n",
        "- Model comparison and interpretability\n",
        "- Real-world AI applications\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üì¶ Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Workshop modules\n",
        "from data_utils import EuroSATDataset, HydroFloodDataset, EUROSAT_CLASSES\n",
        "from models import SatelliteCNN, SatelliteViT, FloodCNN, FloodViT, ModelComparator, create_models\n",
        "from visualization import CNNVisualizer, TransformerVisualizer, ModelComparator as VisComparator\n",
        "from training import WorkshopTrainer, evaluate_model, compare_models, create_dummy_history\n",
        "\n",
        "# Styling\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üéØ Workshop setup complete!\")\n",
        "print(f\"üì± PyTorch version: {torch.__version__}\")\n",
        "print(f\"üñ•Ô∏è  Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üõ∞Ô∏è Part 1: Dataset Exploration (15 minutes)\n",
        "\n",
        "### Dataset Choice: Land-Cover *or* Flood Mapping\n",
        "\n",
        "We have two datasets aligned with GFZ Hydrology research topics:\n",
        "\n",
        "**üåç EuroSAT (Land-Cover)** - Multi-class land use classification\n",
        "- Supports: **Landscape Hydrology** research (infiltration, runoff generation)\n",
        "- Classes: Forest, Agriculture, Urban, Water bodies, etc.\n",
        "\n",
        "**üåä Flood Dataset (Binary)** - Water detection from satellite imagery  \n",
        "- Supports: **Flood Risk & Climate Adaptation** research\n",
        "- Classes: Water vs. No-Water (binary classification)\n",
        "\n",
        "Let's start with satellite imagery classification!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset choice - flip to True for hydrology-centric demo\n",
        "USE_FLOOD = False  # Set to True for flood mapping demo\n",
        "\n",
        "if USE_FLOOD:\n",
        "    print(\"üåä Loading Flood Dataset...\")\n",
        "    dataset = HydroFloodDataset(root_dir=\"../data\", split=\"train\")\n",
        "    NUM_CLASSES = 2\n",
        "    CLASS_NAMES = HydroFloodDataset.LABELS\n",
        "    dataset_type = \"Flood Detection (GFZ: Flood Risk & Climate Adaptation)\"\n",
        "else:\n",
        "    print(\"üõ∞Ô∏è Loading EuroSAT Dataset...\")\n",
        "    dataset = EuroSATDataset(root_dir=\"../data\")\n",
        "    NUM_CLASSES = len(EUROSAT_CLASSES)\n",
        "    CLASS_NAMES = EUROSAT_CLASSES\n",
        "    dataset_type = \"Land-Cover (GFZ: Landscape Hydrology)\"\n",
        "\n",
        "# Get data loaders\n",
        "train_loader, val_loader = dataset.get_dataloaders(batch_size=32)\n",
        "\n",
        "print(f\"\\nüìä Dataset Information:\")\n",
        "print(f\"   üéØ Type: {dataset_type}\")\n",
        "print(f\"   üè∑Ô∏è  Classes: {NUM_CLASSES}\")\n",
        "print(f\"   üì∏ Training batches: {len(train_loader)}\")\n",
        "print(f\"   üì∏ Validation batches: {len(val_loader)}\")\n",
        "print(f\"\\nüè∑Ô∏è  Class names:\")\n",
        "for i, class_name in enumerate(CLASS_NAMES):\n",
        "    print(f\"   {i}: {class_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images\n",
        "print(\"üñºÔ∏è Sample satellite images:\")\n",
        "dataset.visualize_samples(train_loader, num_samples=8)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üß† CNN vs ü§ñ Vision Transformer Demo\n",
        "\n",
        "### Key Differences:\n",
        "- **CNN**: Processes images through convolutional layers with spatial locality\n",
        "- **ViT**: Treats image patches as sequence tokens, uses self-attention\n",
        "- **Feature Maps vs Attention**: CNNs create spatial feature maps, ViTs create attention patterns\n",
        "\n",
        "Let's see both in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create appropriate models based on dataset choice\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üéØ Using device: {device}\")\n",
        "\n",
        "# Determine task type and create optimized models\n",
        "task_type = \"flood\" if USE_FLOOD else \"landcover\"\n",
        "cnn_model, vit_model = create_models(num_classes=NUM_CLASSES, task_type=task_type)\n",
        "\n",
        "# Move models to device\n",
        "cnn_model.to(device)\n",
        "vit_model.to(device)\n",
        "\n",
        "print(f\"\\nüîç Model Architecture Comparison:\")\n",
        "print(f\"   üìä CNN: {cnn_model.__class__.__name__} ({sum(p.numel() for p in cnn_model.parameters()):,} params)\")\n",
        "print(f\"   ü§ñ ViT: {vit_model.__class__.__name__} ({sum(p.numel() for p in vit_model.parameters()):,} params)\")\n",
        "print(f\"   ‚öñÔ∏è  Parameter Ratio: {sum(p.numel() for p in vit_model.parameters()) / sum(p.numel() for p in cnn_model.parameters()):.1f}x\")\n",
        "\n",
        "if USE_FLOOD:\n",
        "    print(f\"\\nüí° Flood Detection Models:\")\n",
        "    print(f\"   üåä FloodCNN: Lightweight architecture optimized for binary water detection\")\n",
        "    print(f\"   üåä FloodViT: Efficient transformer for rapid flood assessment\")\n",
        "else:\n",
        "    print(f\"\\nüí° Land-Cover Models:\")\n",
        "    print(f\"   üåç SatelliteCNN: Full-scale CNN for complex multi-class land-cover\")\n",
        "    print(f\"   üåç SatelliteViT: Full transformer for global land-use patterns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick demo inference to show CNN feature maps and ViT attention\n",
        "sample_data, sample_labels = next(iter(val_loader))\n",
        "sample_data = sample_data.to(device)\n",
        "\n",
        "print(\"üîç Analyzing one satellite image...\")\n",
        "\n",
        "# CNN Feature Maps\n",
        "cnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    cnn_output = cnn_model(sample_data[:1])\n",
        "    feature_maps = cnn_model.get_feature_maps()\n",
        "\n",
        "print(f\"CNN captured {len(feature_maps)} feature map layers\")\n",
        "\n",
        "# ViT Attention\n",
        "vit_model.eval()\n",
        "with torch.no_grad():\n",
        "    vit_output = vit_model(sample_data[:1])\n",
        "    attention_weights = vit_model.get_attention_weights()\n",
        "\n",
        "if attention_weights:\n",
        "    print(f\"ViT captured attention from {len(attention_weights)} layers\")\n",
        "else:\n",
        "    print(\"ViT attention visualization not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize CNN Feature Maps\n",
        "if feature_maps:\n",
        "    print(\"üß† CNN Feature Maps - What does the CNN see?\")\n",
        "    cnn_visualizer = CNNVisualizer(cnn_model)\n",
        "    cnn_visualizer.plot_feature_maps(feature_maps, num_channels=6)\n",
        "    \n",
        "    # Show statistics\n",
        "    cnn_visualizer.plot_activation_statistics(feature_maps)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No CNN feature maps to display\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize ViT Attention\n",
        "if attention_weights:\n",
        "    print(\"ü§ñ Vision Transformer Attention - Where does ViT focus?\")\n",
        "    vit_visualizer = TransformerVisualizer(vit_model)\n",
        "    \n",
        "    # Show attention maps from different layers\n",
        "    print(\"üéØ Early layer attention:\")\n",
        "    vit_visualizer.plot_attention_maps(attention_weights, layer_idx=2)\n",
        "    \n",
        "    print(\"üéØ Final layer attention:\")\n",
        "    vit_visualizer.plot_attention_maps(attention_weights, layer_idx=-1)\n",
        "    \n",
        "    # Attention rollout\n",
        "    print(\"üîç Attention rollout - spatial attention map:\")\n",
        "    rollout_map = vit_visualizer.plot_attention_rollout(attention_weights)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No ViT attention weights to display\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Physics Check Exercise: Do attention patterns make hydrological sense?\n",
        "print(\"üî¨ Physics Interpretability Check\")\n",
        "print(\"   üí≠ Questions to explore:\")\n",
        "print(\"   ‚Ä¢ Do CNN feature maps focus on water bodies, vegetation edges?\")\n",
        "print(\"   ‚Ä¢ Does ViT attention correlate with drainage networks?\")\n",
        "print(\"   ‚Ä¢ Are high-attention regions in topographically relevant areas?\")\n",
        "print(\"   ‚Ä¢ For flood detection: Does the model focus on low-lying areas?\")\n",
        "print()\n",
        "print(\"üí° Next step: Overlay predictions on DEM/terrain data\")\n",
        "print(\"   This helps validate that AI decisions align with physical processes!\")\n",
        "\n",
        "# Workshop participants can discuss:\n",
        "# - How would you validate model focus against known hydrology?\n",
        "# - What additional data layers would improve model physics-awareness?\n",
        "# - How might you incorporate process-based constraints?\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üèÜ Model Comparison & Task-Specific Design\n",
        "\n",
        "### Architecture Choices Based on Problem Complexity\n",
        "\n",
        "**üåç Land-Cover Classification (10 classes)**\n",
        "- **SatelliteCNN**: Deep architecture with 4 blocks, 512 channels max\n",
        "- **SatelliteViT**: Full ViT-Base with 12 layers, 768 hidden dimensions  \n",
        "- **Why?** Complex multi-class problem needs rich feature representations\n",
        "\n",
        "**üåä Flood Detection (2 classes - Binary)**\n",
        "- **FloodCNN**: Lightweight with fewer layers, 256 channels max\n",
        "- **FloodViT**: Efficient transformer with reduced complexity\n",
        "- **Why?** Binary classification can be solved with simpler architectures\n",
        "\n",
        "### üí° Key Lesson: Architecture Complexity Should Match Problem Complexity!\n",
        "\n",
        "| Aspect | CNN | Vision Transformer |\n",
        "|--------|-----|-------------------|\n",
        "| **Architecture** | Convolutional layers | Self-attention mechanism |\n",
        "| **Input Processing** | Spatial convolutions | Patch embeddings + position |\n",
        "| **Visualization** | Feature maps | Attention patterns |\n",
        "| **Inductive Bias** | Translation invariance | None (learned) |\n",
        "| **Global Context** | Limited receptive field | Full global attention |\n",
        "| **Data Efficiency** | Better with small data | Needs larger datasets |\n",
        "\n",
        "### üåä GFZ Hydrology Research Applications\n",
        "\n",
        "**üèûÔ∏è Landscape Hydrology (Plot/Hillslope Scale)**\n",
        "- CNN advantage: Local spatial patterns (vegetation, soil texture)\n",
        "- ViT advantage: Global watershed context, long-range dependencies\n",
        "\n",
        "**üåä Flood Risk & Climate Adaptation**  \n",
        "- CNN: Identify local flood signatures, channel morphology\n",
        "- ViT: Regional flood extent, basin-wide inundation patterns\n",
        "\n",
        "**üìè Hydrogravimetry & Large-Scale Hydrology**\n",
        "- CNN: Process GRACE/GRACE-FO signals with spatial locality\n",
        "- ViT: Connect continental water storage patterns\n",
        "\n",
        "**‚ö° Hydrological Extremes**\n",
        "- CNN: Detect local anomalies in precipitation/temperature fields  \n",
        "- ViT: Capture teleconnections, atmospheric rivers\n",
        "\n",
        "### üéØ Model Selection Strategy\n",
        "- **Binary Classification**: Start with lightweight models (FloodCNN/FloodViT)\n",
        "- **Multi-class Problems**: Use full-scale architectures (SatelliteCNN/SatelliteViT)\n",
        "- **CNN**: Small datasets, need spatial inductive bias, computational constraints, local processes\n",
        "- **ViT**: Large datasets, need global context, sufficient computational resources, basin-scale phenomena\n",
        "- **Hybrid**: Best of both worlds (ConViT, ResT, etc.) - ideal for multi-scale hydrology!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick HuggingFace demo\n",
        "print(\"ü§ó HuggingFace Integration Demo\")\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "    \n",
        "    # Create image classification pipeline\n",
        "    classifier = pipeline(\"image-classification\", \n",
        "                         model=\"google/vit-base-patch16-224\")\n",
        "    \n",
        "    print(\"‚úÖ HuggingFace ViT pipeline ready!\")\n",
        "    print(\"üí° You can now classify any image with just a few lines of code!\")\n",
        "    print(\"Example: predictions = classifier(your_image)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è HuggingFace pipeline demo not available: {e}\")\n",
        "    print(\"üí° In practice, you'd have access to thousands of pre-trained models!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Workshop Summary & Next Steps\n",
        "\n",
        "### What We Covered ‚úÖ\n",
        "1. **Real Dataset**: Satellite imagery classification (not just flowers/digits!)\n",
        "2. **CNN Architecture**: Custom implementation with feature visualization\n",
        "3. **Vision Transformers**: HuggingFace integration with attention maps\n",
        "4. **Visualization**: Both feature maps and attention patterns\n",
        "5. **Comparison**: Understanding when to use each approach\n",
        "6. **HuggingFace**: Easy access to state-of-the-art models\n",
        "\n",
        "### Key Takeaways üí°\n",
        "- **Visualization is crucial** for understanding what models learn\n",
        "- **Different architectures** have different strengths and use cases\n",
        "- **Pre-trained models** (HuggingFace) accelerate development\n",
        "- **Satellite imagery** represents real-world AI applications\n",
        "- **Both CNN and ViT** have important roles in computer vision\n",
        "\n",
        "### üöÄ Next Steps\n",
        "- Try with larger datasets (ImageNet, COCO)\n",
        "- Explore hybrid architectures (ConViT, CoAtNet)\n",
        "- Implement object detection (DETR, YOLO)\n",
        "- Build production pipelines\n",
        "- Experiment with multimodal models (CLIP)\n",
        "\n",
        "---\n",
        "\n",
        "**Questions? Let's discuss! üí≠**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèÉ‚Äç‚ôÄÔ∏è Quick Challenges (If you finish early!)\n",
        "\n",
        "print(\"üèÜ Challenge 1: Architecture-Task Matching\")\n",
        "print(\"   Try switching between FloodCNN/FloodViT and SatelliteCNN/SatelliteViT\")\n",
        "print(\"   - Set USE_FLOOD=True vs USE_FLOOD=False and compare model sizes\")\n",
        "print(\"   - Notice how simpler tasks use simpler architectures!\")\n",
        "\n",
        "print(\"\\nüèÜ Challenge 2: Attention Analysis\")\n",
        "print(\"   Compare attention patterns from different ViT layers\")\n",
        "print(\"   - Use vit_visualizer.plot_attention_maps() with different layer_idx\")\n",
        "\n",
        "print(\"\\nüèÜ Challenge 3: Transfer Learning\")\n",
        "print(\"   Implement fine-tuning with frozen base layers\")\n",
        "print(\"   - Freeze ViT base, only train classification head\")\n",
        "\n",
        "print(\"\\nüèÜ Challenge 4: Ensemble Methods\")\n",
        "print(\"   Combine CNN and ViT predictions\")\n",
        "print(\"   - Average their outputs for better performance\")\n",
        "\n",
        "print(\"\\nüèÜ Challenge 5: Real Dataset\")\n",
        "print(\"   Replace CIFAR-10 with actual EuroSAT dataset\")\n",
        "print(\"   - Download from: https://github.com/phelber/EuroSAT\")\n",
        "\n",
        "print(\"\\nüèÜ Challenge 6: GFZ Data Integration\")  \n",
        "print(\"   Download satellite data from GFZ's Geoportal\")\n",
        "print(\"   - Use ogr2ogr to extract your study area\")\n",
        "print(\"   - Test models on actual German catchments\")\n",
        "\n",
        "print(\"\\nüèÜ Challenge 7: Hybrid CNN-Physics Models\")\n",
        "print(\"   Combine AI outputs with process-based models\")\n",
        "print(\"   - Use CNN land-cover as input to rainfall-runoff models\")\n",
        "print(\"   - Constrain flood predictions with DEM flow directions\")\n",
        "\n",
        "print(\"\\nüí° Most importantly: Experiment and have fun! üéâ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
